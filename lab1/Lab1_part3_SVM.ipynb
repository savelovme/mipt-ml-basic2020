{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab assignment №1, part 3\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*This is the third part of the assignment. First and second parts are waiting for you in the same directory.*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T23:26:22.240114Z",
     "start_time": "2019-03-13T23:26:21.327520Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57f562bf4f554fae",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 3. SVM and kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7b8f71403aa9084",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Kernels concept get adopted in variety of ML algorithms (e.g. Kernel PCA, Gaussian Processes, kNN, ...).\n",
    "\n",
    "So in this task you are to examine kernels for SVM algorithm applied to rather simple artificial datasets.\n",
    "\n",
    "To make it clear: we will work with the classification problem through the whole notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b128784928e8df1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's generate our dataset and take a look on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T23:26:22.247247Z",
     "start_time": "2019-03-13T23:26:22.242895Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee8cf8e9cf114b9d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "moons_points, moons_labels = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "\n",
    "plt.scatter(moons_points[:, 0], moons_points[:, 1], c=moons_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35b09404d22ab9f4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 1.1 Pure models.\n",
    "First let's try to solve this case with good old Logistic Regression and simple (linear kernel) SVM classifier.\n",
    "\n",
    "Train LR and SVM classifiers (choose params by hand, no CV or intensive grid search neeeded) and plot their decision regions. Calculate one preffered classification metric.\n",
    "\n",
    "Describe results in one-two sentences.\n",
    "\n",
    "_Tip:_ to plot classifiers decisions you colud use either sklearn examples ([this](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py) or any other) and mess with matplotlib yourself or great [mlxtend](https://github.com/rasbt/mlxtend) package (see their examples for details)\n",
    "\n",
    "_Pro Tip:_ write function `plot_decisions` taking a dataset and an estimator and plotting the results cause you want to use it several times below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для отрисовки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decisions(fig, X, y, estimator, row, col, prob=False):\n",
    "    '''\n",
    "    Plot decision region for X by the 0 and 1 features using estimator (classification problem)\n",
    "    If we have classification problem into more than 2 classes, arg prob ignored\n",
    "    '''\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    h = 0.01\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = estimator.predict(np.hstack((np.expand_dims(xx.ravel(), axis=-1),\n",
    "                          np.expand_dims(yy.ravel(), axis=-1))))\n",
    "    \n",
    "    if len(np.unique(y)) == 2 and prob:\n",
    "        Z = estimator.predict_proba(np.hstack((np.expand_dims(xx.ravel(), axis=-1),\n",
    "                                    np.expand_dims(yy.ravel(), axis=-1))))[:, 0]\n",
    "    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    fig.add_trace(go.Contour(z = Z, \n",
    "                             x = np.arange(x_min, x_max, h), \n",
    "                             y = np.arange(y_min, y_max, h), \n",
    "                             colorscale = 'Viridis',\n",
    "                             showscale=False), row=row, col=col)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=X[:, 0], \n",
    "                             y=X[:, 1],\n",
    "                             mode='markers',\n",
    "                             marker=dict(size=5, color=y, colorscale='plotly3')), row=row, col=col)\n",
    "    \n",
    "    fig.update_xaxes(title_text='Accuracy: {}'.format(accuracy_score(y, estimator.predict(X))),\n",
    "                     row=row, col=col)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T23:26:22.846438Z",
     "start_time": "2019-03-13T23:26:22.482543Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-550546e70e191bc3",
     "locked": false,
     "points": 10,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = 0.1, max_iter=500)\n",
    "svm = SVC(kernel='linear', C=0.1, probability=True)\n",
    "\n",
    "lr.fit(moons_points, moons_labels)\n",
    "svm.fit(moons_points, moons_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows = 1,\n",
    "    cols = 2,\n",
    "    subplot_titles=['Decision region, {}'.format(md) for md in ('svm', 'logreg')]\n",
    ")\n",
    "\n",
    "plot_decisions(fig, moons_points, moons_labels, svm, 1, 1, prob=True)\n",
    "plot_decisions(fig, moons_points, moons_labels, lr, 1, 2, prob=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize = False,\n",
    "    width = 950,\n",
    "    height = 400,\n",
    "    showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По самим разделяющим поверхностям трудно понять, какая лучше. Они обе примерный угол наилучшей линейной разделяющей поверхности.  \n",
    "Точность чуть выше у svm, но тут не так критично - да и обе модели провели прямую, только оптимизировали ее проведение с помощью разных loss-в.  \n",
    "Для более интересных результатов, будет изменять ядро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Kernel tirck\n",
    "\n",
    "\n",
    "Now use different kernels (`poly`, `rbf`, `sigmoid`) on SVC to get better results. Play `degree` parameter and others.\n",
    "\n",
    "For each kernel estimate optimal params, plot decision regions, calculate metric you've chosen eariler.\n",
    "\n",
    "Write couple of sentences on:\n",
    "\n",
    "* What have happenned with classification quality?\n",
    "* How did decision border changed for each kernel?\n",
    "* What `degree` have you chosen and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T23:26:22.864832Z",
     "start_time": "2019-03-13T23:26:22.862013Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3a1681e6d52ed236",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'clf__C' : np.linspace(0.0001, 10, 100),\n",
    "              'clf__gamma' : np.linspace(0, 1, 5)}\n",
    "\n",
    "pipe = Pipeline([('clf', SVC(kernel='rbf', probability=True))])\n",
    "svm_rbf = GridSearchCV(pipe, param_grid, n_jobs=-1, verbose=5).fit(moons_points, moons_labels).best_estimator_\n",
    "\n",
    "param_grid = {'clf__C' : np.linspace(0.0001, 10, 100),\n",
    "              'clf__gamma' : np.linspace(0, 1, 5),\n",
    "              'clf__coef0' : [0, 0.5, 1]}\n",
    "pipe = Pipeline([('clf', SVC(kernel='sigmoid', probability=True))])\n",
    "svm_sigm = GridSearchCV(pipe, param_grid, n_jobs=-1, verbose=5).fit(moons_points, moons_labels).best_estimator_\n",
    "\n",
    "param_grid = {'clf__C' : np.linspace(0.0001, 10, 100),\n",
    "              'clf__degree': np.arange(2, 10),\n",
    "              'clf__coef0' : [0, 0.5, 1]}\n",
    "\n",
    "pipe = Pipeline([('clf', SVC(kernel='poly', probability=True))])\n",
    "svm_poly = GridSearchCV(pipe, param_grid, n_jobs=-1, verbose=5).fit(moons_points, moons_labels).best_estimator_\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows = 2,\n",
    "    cols = 2,\n",
    "    subplot_titles=['Decision region, {}'.format(md) for md in ('svm rbf',\n",
    "                                                                'svm sigmoid',\n",
    "                                                                'svm poly')]\n",
    ")\n",
    "\n",
    "plot_decisions(fig, moons_points, moons_labels, svm_rbf, 1, 1, prob=True)\n",
    "plot_decisions(fig, moons_points, moons_labels, svm_sigm, 1, 2, prob=True)\n",
    "plot_decisions(fig, moons_points, moons_labels, svm_poly, 2, 1, prob=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize = False,\n",
    "    width = 900,\n",
    "    height = 900,\n",
    "    showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly.named_steps['clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, наихудшим образом приблизило сигмоидальное ядро (даже хуже, чем линейное).  \n",
    "Нормальное и полиномиальные ядра же показали примерно одинаковую точность. Но если приглядеться к графикам, то в случае нормального ядра класс 0 окружен классом 1, и подобное поведение выглядит странным, в то же время полиномиальное ядро разделяет два класса и не один другим, проводит кривую между ними.  \n",
    "Оптимальная тепень многочлена равна 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba9a59e3ec57f514",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 1.3 Simpler solution (of a kind)\n",
    "What is we could use Logisitc Regression to successfully solve this task?\n",
    "\n",
    "Feature generation is a thing to help here. Different techniques of feature generation are used in real life, couple of them will be covered in additional lectures.\n",
    "\n",
    "In particular case simple `PolynomialFeatures` ([link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)) are able to save the day.\n",
    "\n",
    "Generate the set of new features, train LR on it, plot decision regions, calculate metric.\n",
    "\n",
    "* Comare SVM's results with this solution (quality, borders type)\n",
    "* What degree of PolynomialFeatures have you used? Compare with same SVM kernel parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T23:26:22.869584Z",
     "start_time": "2019-03-13T23:26:22.866757Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-58a1e03cab2ca349",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "degrees = [2, 4, 6, 8, 10, 12]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows = 3,\n",
    "    cols = 2,\n",
    "    subplot_titles=['Decision region, {}'.format(md) for md in ['logreg degree {}'.format(i) for i in degrees]]\n",
    ")\n",
    "\n",
    "param_grid = {'clf__C' : np.linspace(0.0001, 10, 100)}\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "\n",
    "    pipe = Pipeline([('poly', PolynomialFeatures(degree=degree)), \n",
    "                     ('clf', LogisticRegression(fit_intercept=False))])\n",
    "\n",
    "    log_reg = GridSearchCV(pipe, param_grid, n_jobs=-1).fit(moons_points, moons_labels).best_estimator_\n",
    "    \n",
    "    plot_decisions(fig, moons_points, moons_labels, log_reg, int(i / 2) + 1, int(i % 2) + 1, prob=True)\n",
    "    \n",
    "fig.update_layout(\n",
    "    autosize = False,\n",
    "    width = 900,\n",
    "    height = 900,\n",
    "    showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже на степени, равной 6, нам удалось добиться точности не хуже, чем дал SVM.  \n",
    "Т.е. действительно можно обучать логистическую регрессию над преобразованными нашими признаками, и ожидать прирост точности за счет искуственного создания нелинейных зависимостей от исходных.  \n",
    "Но очевидно не стоит создать признаки, являющиеся линейной комбинацией исходных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-868839a4a8358c59",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 1.4 Harder problem\n",
    "\n",
    "Let's make this task a bit more challenging via upgrading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T23:26:23.084319Z",
     "start_time": "2019-03-13T23:26:22.876842Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86be614f32559cea",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "circles_points, circles_labels = make_circles(n_samples=500, noise=0.06, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles_points[:, 0], circles_points[:, 1], c=circles_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7e5a8e0da66afbe",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "And even more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T23:26:23.326325Z",
     "start_time": "2019-03-13T23:26:23.086480Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a98ef8e43822e61",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "points = np.vstack((circles_points*2.5 + 0.5, moons_points))\n",
    "labels = np.hstack((circles_labels, moons_labels + 2)) # + 2 to distinct moons classes\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(points[:, 0], points[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7c2a785a2d63ce73",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now do your best using all the approaches above!\n",
    "\n",
    "Tune LR with generated features, SVM with appropriate kernel of your choice. You may add some of your loved models to demonstrate their (and your) strength. Again plot decision regions, calculate metric.\n",
    "\n",
    "Justify the results in a few phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим все те же 3 модели, посмотрим на accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T23:26:23.330584Z",
     "start_time": "2019-03-13T23:26:23.328232Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e61b36ea61909c83",
     "locked": false,
     "points": 40,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'clf__C' : np.linspace(0.0001, 10, 10),\n",
    "              'clf__gamma' : np.linspace(0, 1, 5)}\n",
    "\n",
    "pipe = Pipeline([('clf', SVC(kernel='rbf', probability=True))])\n",
    "svm_rbf = GridSearchCV(pipe, param_grid, n_jobs=-1).fit(points, labels).best_estimator_\n",
    "\n",
    "param_grid = {'clf__C' : np.linspace(0.0001, 10, 10),\n",
    "              'clf__degree': np.arange(2, 8, 2),\n",
    "              'clf__coef0' : [0, 1]}\n",
    "\n",
    "pipe = Pipeline([('clf', SVC(kernel='poly', probability=True))])\n",
    "svm_poly = GridSearchCV(pipe, param_grid, n_jobs=-1, verbose=5).fit(points, labels).best_estimator_\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows = 1,\n",
    "    cols = 2,\n",
    "    subplot_titles=['Decision region, {}'.format(md) for md in ('svm rbf',\n",
    "                                                                'svm sigmoid',\n",
    "                                                                'svm poly')]\n",
    ")\n",
    "\n",
    "plot_decisions(fig, points, labels, svm_rbf, 1, 1, prob=True)\n",
    "plot_decisions(fig, points, labels, svm_poly, 1, 2, prob=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize = False,\n",
    "    width = 900,\n",
    "    height = 400,\n",
    "    showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим логистическую регрессию с полиномиальными фичами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [2, 4, 6, 8, 10, 12]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows = 1,\n",
    "    cols = 1,\n",
    "    subplot_titles=['Decision region, {}'.format('logreg')]\n",
    ")\n",
    "\n",
    "param_grid = {'clf__C' : np.linspace(0.1, 3, 10),\n",
    "              'poly__degree': degrees}\n",
    "\n",
    "pipe = Pipeline([('poly', PolynomialFeatures()), \n",
    "                 ('clf', LogisticRegression(fit_intercept=False))])\n",
    "\n",
    "log_reg = GridSearchCV(pipe, param_grid, n_jobs=-1, verbose=5).fit(points, labels).best_estimator_\n",
    "    \n",
    "plot_decisions(fig, points, labels, log_reg, 1, 1, prob=False)\n",
    "    \n",
    "fig.update_layout(\n",
    "    autosize = False,\n",
    "    width = 450,\n",
    "    height = 400,\n",
    "    showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще можно заметить, что наш оптимизатор не смог сойтись, но при этом все равно хорошая точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows = 1,\n",
    "    cols = 1,\n",
    "    subplot_titles=['Decision region, {}'.format('random forest')]\n",
    ")\n",
    "\n",
    "param_grid = {'poly__degree': [2, 3, 4],\n",
    "              'clf__max_depth': [2, 3, 5, 10, 15, 20, 25, 50],\n",
    "              'clf__max_features': [3, 5, 7, 10, 20, 40]}\n",
    "\n",
    "\n",
    "pipe = Pipeline([('poly', PolynomialFeatures()),\n",
    "                     ('clf', RandomForestClassifier())])\n",
    "\n",
    "model_rf = GridSearchCV(pipe, param_grid, n_jobs=-1, verbose=5).fit(points, labels).best_estimator_\n",
    "\n",
    "model_rf.fit(points, labels)\n",
    "\n",
    "plot_decisions(fig, points, labels, model_rf, 1, 1, prob=False)\n",
    "    \n",
    "fig.update_layout(\n",
    "    autosize = False,\n",
    "    width = 450,\n",
    "    height = 400,\n",
    "    showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А Random Forest у нас переобучился под выборку, можно заметить по достаточно узким полоскам на графике. При этом он все равно уловил основную зависимость между меткой класса и положением точек в пространстве. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Вывод.***  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно снова заметить, что логистическая регрессия с полиномиальными фичами работает примерно так же как svm с нелинейным ядром. При этом на более сложном датасете точность классификации конечно же упало, но тут очень сильно повлияло наложение классов, идеально разделить было бы трудно.  \n",
    "Случайный лес же переобучается, по графику видно, что он хоть и улавливает зависимости, но реагирует на шум, из-за этого общая разделяющая поверхность становится более грубой.  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
